import google.generativeai as genai
import pandas as pd
import time
import sys
import os
from google.api_core import exceptions

# ================= é…ç½®åŒºåŸŸ =================
try:
    import config
    API_KEY = config.API_KEY
except ImportError:
    print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° 'config.py' æ–‡ä»¶ã€‚")
    sys.exit(1)

INPUT_FILE = "åŠæ—¶é›¨é«˜è€ƒè‹±è¯­è¯æ±‡æ‰‹å†Œ.csv"
OUTPUT_FILE = "anki_output_complete.csv"

# æ‰¹å¤„ç†å¤§å° (å»ºè®®ä¿æŒåœ¨ 10ï¼Œå¤ªå¤§ä¼šå¢åŠ è¶…æ—¶é£é™©)
BATCH_SIZE = 10 
# ===========================================

# é…ç½® Gemini
genai.configure(api_key=API_KEY)

# ã€é‡è¦ä¿®æ”¹ã€‘æ”¹å› gemini-2.0-flashï¼Œå®ƒçš„é…é¢é€šå¸¸æ¯” 2.5 å®½æ¾
model = genai.GenerativeModel('gemini-2.0-flash')

def get_anki_content_with_retry(words_list, max_retries=5):
    """
    å¸¦è‡ªåŠ¨é‡è¯•åŠŸèƒ½çš„ API è°ƒç”¨å‡½æ•°
    """
    words_str = ", ".join(words_list)
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯­è¯æ±‡è€å¸ˆã€‚è¯·ä¸ºä»¥ä¸‹å•è¯ç”Ÿæˆ Anki å¡ç‰‡å†…å®¹ï¼š
    å•è¯åˆ—è¡¨: {words_str}

    è¯·ä¸¥æ ¼æŒ‰ç…§ CSV æ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«è¡¨å¤´ï¼Œä¸è¦ä½¿ç”¨ä»£ç å—æ ‡è®°ï¼Œæ¯ä¸€è¡ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    Word|IPA|Sentence|Mnemonic

    è¦æ±‚ï¼š
    1. Word: å•è¯æœ¬èº«
    2. IPA: å•è¯çš„éŸ³æ ‡ï¼ˆå¦‚ /hÃ¦k/ï¼‰
    3. Sentence: ä¸€ä¸ªç®€çŸ­ã€åœ°é“çš„è‹±æ–‡ä¾‹å¥ï¼Œå¹¶åœ¨æ‹¬å·å†…é™„å¸¦ä¸­æ–‡ç¿»è¯‘ã€‚
    4. Mnemonic: è¯æ ¹è¯ç¼€è§£ææˆ–è°éŸ³åŠ©è®°ã€‚
    5. åˆ†éš”ç¬¦: è¯·åŠ¡å¿…ä½¿ç”¨ç«–çº¿ '|' ä½œä¸ºå­—æ®µåˆ†éš”ç¬¦ï¼Œä¸è¦ä½¿ç”¨é€—å·ã€‚
    6. å¦‚æœå•è¯æœ‰å¤šä¸ªé‡Šä¹‰ï¼Œä¸è¦åœ¨è¾“å‡ºä¸­åŒ…å«é‡Šä¹‰ï¼Œåªéœ€å¤„ç†æˆ‘è¦æ±‚çš„å­—æ®µã€‚
    """

    for attempt in range(max_retries):
        try:
            response = model.generate_content(prompt)
            return response.text.strip()
            
        except exceptions.ResourceExhausted:
            # é‡åˆ° 429 é…é¢è¶…é™é”™è¯¯
            wait_time = 60 * (attempt + 1) # ç¬¬ä¸€æ¬¡ç­‰60ç§’ï¼Œç¬¬äºŒæ¬¡ç­‰120ç§’...
            print(f"\nâš ï¸ é…é¢å·²æ»¡ (429 Error)ï¼Œè„šæœ¬å°†æš‚åœ {wait_time} ç§’åè‡ªåŠ¨é‡è¯• ({attempt + 1}/{max_retries})...")
            time.sleep(wait_time)
            continue
            
        except Exception as e:
            print(f"\nâŒ å…¶ä»– API é”™è¯¯: {e}")
            # å¦‚æœæ˜¯å…¶ä»–ä¸¥é‡é”™è¯¯ï¼Œç¨å¾®ç­‰ä¸€ä¸‹å†è¯•
            time.sleep(5)
            continue
    
    print("\nâŒ é‡è¯•å¤šæ¬¡å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡ã€‚")
    return None

def process_csv():
    # 1. è¯»å–æ–‡ä»¶
    if not os.path.exists(INPUT_FILE):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{INPUT_FILE}'")
        return

    # æ™ºèƒ½è¯»å–ï¼šå…ˆå°è¯•æ— è¡¨å¤´ï¼Œè‹¥å¤±è´¥åˆ™å°è¯•æœ‰è¡¨å¤´
    try:
        df = pd.read_csv(INPUT_FILE, header=None, names=['Word', 'Definition'], on_bad_lines='skip')
        # ç®€å•æ£€æŸ¥ï¼šå¦‚æœç¬¬ä¸€è¡Œçœ‹èµ·æ¥åƒè¡¨å¤´ï¼ˆæ¯”å¦‚ Word è¿™ä¸€åˆ—çš„å€¼å°±æ˜¯ "Word"ï¼‰ï¼Œåˆ™é‡æ–°è¯»å–
        if df.iloc[0]['Word'] == 'Word' or df.iloc[0]['Word'] == 'æ­£é¢ (Word)':
             df = pd.read_csv(INPUT_FILE)
             # ç»Ÿä¸€åˆ—å
             if 'æ­£é¢ (Word)' in df.columns:
                 df.rename(columns={'æ­£é¢ (Word)': 'Word', 'èƒŒé¢é‡Šä¹‰ (Definition)': 'Definition'}, inplace=True)
    except Exception:
        df = pd.read_csv(INPUT_FILE)

    # 2. æ£€æŸ¥æ˜¯å¦å·²æœ‰éƒ¨åˆ†è¿›åº¦ (æ–­ç‚¹ç»­ä¼ åŠŸèƒ½)
    # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼Œæˆ‘ä»¬è¯»å–å®ƒï¼Œçœ‹çœ‹å¤„ç†äº†å¤šå°‘ä¸ª
    processed_words = set()
    if os.path.exists(OUTPUT_FILE):
        try:
            df_existing = pd.read_csv(OUTPUT_FILE)
            if 'æ­£é¢ (Word)' in df_existing.columns:
                processed_words = set(df_existing['æ­£é¢ (Word)'].astype(str).tolist())
            print(f"ğŸ“‚ æ£€æµ‹åˆ°å·²å­˜åœ¨è¾“å‡ºæ–‡ä»¶ï¼ŒåŒ…å« {len(processed_words)} ä¸ªå•è¯ã€‚è„šæœ¬å°†è‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„å•è¯ã€‚")
        except:
            pass

    # æ•°æ®æ¸…æ´—
    df['Word'] = df['Word'].astype(str).str.strip()
    df = df.dropna(subset=['Word'])
    df = df[df['Word'] != 'nan']
    
    # è¿‡æ»¤æ‰å·²ç»å¤„ç†è¿‡çš„å•è¯
    df_to_process = df[~df['Word'].isin(processed_words)].copy()
    
    all_words = df_to_process['Word'].unique().tolist()
    total_words = len(all_words)
    
    if total_words == 0:
        print("âœ… æ‰€æœ‰å•è¯éƒ½å·²å¤„ç†å®Œæ¯•ï¼")
        return

    print(f"ğŸš€ å¼€å§‹å¤„ç†å‰©ä½™çš„ {total_words} ä¸ªå•è¯ï¼ˆæ¨¡å‹: gemini-2.0-flashï¼‰...")

    # å‡†å¤‡ç»“æœå®¹å™¨
    results = []
    
    # åˆ†æ‰¹å¤„ç†
    for i in range(0, total_words, BATCH_SIZE):
        batch_words = all_words[i : i + BATCH_SIZE]
        print(f"æ­£åœ¨å¤„ç†: {batch_words[0]} ... ({i+1}/{total_words})")
        
        # è°ƒç”¨ API (å¸¦é‡è¯•)
        api_output = get_anki_content_with_retry(batch_words)
        
        current_batch_results = []
        if api_output:
            lines = api_output.split('\n')
            for line in lines:
                parts = line.split('|')
                if len(parts) >= 4:
                    current_batch_results.append({
                        'Word': parts[0].strip(),
                        'IPA': parts[1].strip(),
                        'Sentence': parts[2].strip(),
                        'Mnemonic': parts[3].strip()
                    })
        
        # === å®æ—¶ä¿å­˜ (å…³é”®ä¿®æ”¹) ===
        # æ¯å¤„ç†å®Œä¸€æ‰¹ï¼Œå°±ç«‹å³ä¿å­˜åˆ°æ–‡ä»¶ï¼Œé˜²æ­¢ç¨‹åºä¸­æ–­æ•°æ®ä¸¢å¤±
        if current_batch_results:
            df_batch_res = pd.DataFrame(current_batch_results)
            # æ‰¾åˆ°åŸå§‹ Definition
            df_batch_merged = pd.merge(df[df['Word'].isin(batch_words)], df_batch_res, on='Word', how='inner')
            
            # æ ¼å¼åŒ–
            def format_row(row):
                definition = str(row['Definition']).replace('\n', '<br>').replace('\r', '')
                ipa = str(row['IPA']) if pd.notna(row['IPA']) else ""
                if ipa and ipa not in definition:
                    definition = f"[{ipa}]<br>{definition}"
                return pd.Series({
                    'æ­£é¢ (Word)': row['Word'],
                    'èƒŒé¢é‡Šä¹‰ (Definition)': definition,
                    'ä¾‹å¥ (Sentence)': row['Sentence'],
                    'è¯æ ¹/åŠ©è®° (Mnemonic)': row['Mnemonic'],
                    'éš¾åº¦æ ‡ç­¾ (Tag)': 'é«˜è€ƒè‹±è¯­è¯æ±‡'
                })

            final_batch_output = df_batch_merged.apply(format_row, axis=1)
            
            # è¿½åŠ å†™å…¥æ¨¡å¼ (Append Mode)
            # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å†™ï¼ˆä¸”æ–‡ä»¶ä¸å­˜åœ¨ï¼‰ï¼Œå†™è¡¨å¤´ï¼›å¦åˆ™ä¸å†™è¡¨å¤´
            header_needed = not os.path.exists(OUTPUT_FILE)
            final_batch_output.to_csv(OUTPUT_FILE, mode='a', index=False, header=header_needed)
            
        # ç¨å¾®ä¼‘çœ ä¸€ä¸‹ï¼Œç»™ API å–˜æ¯æ—¶é—´
        time.sleep(2) 

    print(f"\nâœ… å…¨éƒ¨å®Œæˆï¼æ–‡ä»¶å·²æ›´æ–°è‡³: {OUTPUT_FILE}")

if __name__ == "__main__":
    process_csv()